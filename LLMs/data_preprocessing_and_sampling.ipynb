{"cells":[{"cell_type":"markdown","metadata":{},"source":["The data used by LLMs can be split in to smaller subwords and encoded into a continuous space (vectors), a process referred as embedding.\n","Some examples of embedding include: \n","- Retrieval-augmented generation (RAG).\n","- Word embeding (word2vec)\n","\n","The embedding size varies depending on the specific foundation model. For instance, GPT-2 uses an embedding size of 768 dimensions."]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizing"]},{"cell_type":"code","execution_count":3,"metadata":{"metadata":{}},"outputs":[],"source":["from importlib.metadata import version\n","import tiktoken\n","import torch"]},{"cell_type":"code","execution_count":4,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Total characters: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"]}],"source":["with open(\"./data/example_text.txt\",\"r\",encoding=\"utf-8\") as f:\n","    raw = f.read()\n","    \n","print(\"Total characters:\",len(raw))\n","print(raw[:100])"]},{"cell_type":"code","execution_count":8,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"]}],"source":["import re\n","preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)',raw)\n","preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","print(preprocessed[:50])"]},{"cell_type":"markdown","metadata":{},"source":["List all unique tokens"]},{"cell_type":"code","execution_count":64,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary size: 1161\n","First words: ['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Carlo;', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindle:', 'Grindles', 'HAD', 'Had', 'Hang', 'Has']\n"]}],"source":["all_tokens = sorted(list(set(preprocessed)))\n","# Extend vocabulary to deal with unknown words (not existing in the training vocabulary, and end of file symbols to distinguish between different text sections):\n","all_tokens.extend([\"<|unk|>\",\"<|endoftext|>\"])\n","\n","vocabulary_size = len(all_tokens)\n","print(f\"Vocabulary size: {vocabulary_size}\")\n","print(\"First words:\",all_words[:50])"]},{"cell_type":"code","execution_count":70,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["('younger', 1156)\n","('your', 1157)\n","('yourself', 1158)\n","('<|unk|>', 1159)\n","('<|endoftext|>', 1160)\n"]}],"source":["vocabulary = {token:integer for integer,token in enumerate(all_tokens)}\n","\n","# Print last entries of the vocabulary (including unknown words and end of text)\n","for i,item in enumerate(list(vocabulary.items())[-5:]):\n","    print(item)\n"]},{"cell_type":"markdown","metadata":{},"source":["Simple text tokenizer"]},{"cell_type":"code","execution_count":71,"metadata":{"metadata":{}},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {i:s for s,i in vocab.items()}\n","    \n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n","        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","        preprocessed = [item if item in self.str_to_int\n","                        else \"<|unk|>\" for item in preprocessed] #Replace unknown words by the \"<|unk|>\" token\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","        \n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # Replace spaces before the specified punctuations\n","        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","        return text"]},{"cell_type":"markdown","metadata":{},"source":["Instantiate the tokenizer class object and tokenize a text of interest"]},{"cell_type":"code","execution_count":79,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Input text: \n","at, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on Hello <|endoftext|>\n","Tokenized text:\n","[185, 5, 579, 1013, 546, 738, 559, 504, 5, 541, 522, 377, 559, 766, 5, 676, 119, 862, 1129, 5, 162, 404, 557, 579, 119, 1093, 743, 1159, 1160]\n","Decoding token:\n","at, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on <|unk|> <|endoftext|>\n"]}],"source":["tokenizer = Tokenizer(vocabulary) # Instantiate Tokenizer object\n","text = \" \".join([raw[130:250],\"Hello\"]) # Include a word non-existing in the vocabulary, i.e. the word 'Hello' doesn't exist in the original input text.\n","text = \" \".join([text,\"<|endoftext|>\"]) # Include the End of Text character to specify the end of the input text to be tokenized (useful for concatenating multiple, un-related texts)\n","\n","print(f\"Input text: \\n{text}\")\n","\n","ids = tokenizer.encode(text)\n","print(f\"Tokenized text:\\n{ids}\")\n","print(\"Decoding token:\")\n","print(tokenizer.decode(ids))\n"]},{"cell_type":"markdown","metadata":{},"source":["Tokenization using Byte pair encoding"]},{"cell_type":"code","execution_count":81,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["tiktoken ver= 0.6.0\n"]}],"source":["from importlib_metadata import version\n","import tiktoken\n","print(\"tiktoken ver=\", version(\"tiktoken\"))"]},{"cell_type":"code","execution_count":82,"metadata":{"metadata":{}},"outputs":[],"source":["tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiate BPE tokenized (byte pair encoding)"]},{"cell_type":"code","execution_count":101,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenized text:[630, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 220, 50256, 326, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 275, 1052, 20035, 26449]\n","Decoded token:reat surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would <|endoftext|> that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; b AnUnknownWord\n"]}],"source":["text = \" \".join([raw[100:297],\"<|endoftext|>\",raw[355:550]]) # Concatenate two batches of text\n","text = \" \".join([text,\"AnUnknownWord\"])\n","integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n","print(f\"Tokenized text:{integers}\")\n","strings = tokenizer.decode(integers)\n","print(f\"Decoded token:{strings}\")\n","\n"," # The BPE tokenizer encodes and decodes unknown words by breaking them down into individual characters."]},{"cell_type":"markdown","metadata":{},"source":["## Data sampling"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":2}
